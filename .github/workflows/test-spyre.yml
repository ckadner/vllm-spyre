name: Test

on:
  push:
  pull_request:
  workflow_dispatch:

env:
  # force output to be colored for non-tty GHA runner shell
  FORCE_COLOR: "1"
  # facilitate testing by building vLLM for CPU when needed
  VLLM_CPU_DISABLE_AVX512: "true"
  VLLM_TARGET_DEVICE: "cpu"
  VLLM_PLUGINS: "spyre"
  # prefer index for torch cpu version
  UV_EXTRA_INDEX_URL: "https://download.pytorch.org/whl/cpu"
  # have uv match pip's behaviour for extra index operations
  UV_INDEX_STRATEGY: "unsafe-best-match"
  VLLM_SPYRE_TEST_MODEL_DIR: "${{ github.workspace }}/models"

concurrency:
  group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  test:
    timeout-minutes: 20
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: ["ubuntu-latest"]
        python_version: ["3.12"]
        vllm_version:
          # skip the pypi version as it will not work on CPU
          - name: "vLLM:v0.8.0"
            repo: "git+https://github.com/vllm-project/vllm --tag v0.8.0"
          - name: "vLLM:main"
            repo: "git+https://github.com/vllm-project/vllm --branch main"
          - name: "ODH:main"
            repo: "git+https://github.com/opendatahub-io/vllm --branch main"
        test_suite:
          - name: "V0"
            tests: "V0 and eager"
            flags: "--timeout=300"
          - name: "V1"
            tests: "(V1- and eager) or test_sampling_metadata_in_input_batch"
            flags: "--timeout=300 --forked"
        exclude:
          - vllm_version:
              name: "vLLM:main"
            test_suite:
              name: "V1"
          - vllm_version:
              name: "ODH:main"
            test_suite:
              name: "V1"

    name: "${{ matrix.test_suite.name }} (${{ matrix.vllm_version.name }})"

    steps:
      - name: "Checkout"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: "Cache the ccache cache directory"
        id: ccache-cache
        uses: actions/cache@v4
        with:
          path: /home/runner/.cache/ccache
          key: ${{ runner.os }}

      - name: "Cache HuggingFace hub cache"
        id: hf-cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/huggingface/hub
          key: ${{ runner.os }}

      - name: "Install build dependencies"
        run: |
          sudo apt update
          sudo apt install --no-install-recommends -y \
            ccache \
            libnuma-dev \
            libdnnl-dev \
            opencl-dev

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python_version }}

      # since GH action runners are one-and-done, we can install all requirements
      # into the system packages and cache and reuse all the installed Python system packages
      - name: Cache dependencies
        uses: actions/cache@v4
        id: python-cache
        with:
          path: ${{ env.pythonLocation }}
          key: ${{ runner.os }}-${{ env.pythonLocation }}-${{ hashFiles('requirements.txt') }}-${{ hashFiles('requirements-test.txt') }}
          restore-keys:
            ${{ runner.os }}-${{ env.pythonLocation }}-

      - name: "Download and install vLLM"
        if: steps.cache.outputs.cache-hit != 'true'
        env:
          PIP_EXTRA_INDEX_URL: https://download.pytorch.org/whl/cpu
        run: |
          git clone --depth 1 https://github.com/vllm-project/vllm.git
          cd vllm
          git fetch --tags
          git checkout v0.8.0
          pip install torch=="2.5.1+cpu" --index-url https://download.pytorch.org/whl/cpu
          python use_existing_torch.py
          pip install -r requirements/build.txt
          SETUPTOOLS_SCM_PRETEND_VERSION=0.8.0 VLLM_TARGET_DEVICE=empty pip install --verbose . --no-build-isolation

      - name: "Install Python dependencies"
        if: steps.cache.outputs.cache-hit != 'true'
        env:
          PIP_EXTRA_INDEX_URL: https://download.pytorch.org/whl/cpu
        run: |
          python -m pip install -r requirements.txt
          python -m pip install -r requirements-test.txt

      - name: "Install vLLM-Spyre plugin"
        run: |
          pip install -v -e .

      - name: "Download HF models"
        run: |
          mkdir -p "${VLLM_SPYRE_TEST_MODEL_DIR}"
          
          python -c "from transformers import pipeline; pipeline(\"text-generation\", model=\"JackFram/llama-160m\")"
          VARIANT=$(ls ~/.cache/huggingface/hub/models--JackFram--llama-160m/snapshots/)
          ln -s ~/.cache/huggingface/hub/models--JackFram--llama-160m/snapshots/"${VARIANT}" "${VLLM_SPYRE_TEST_MODEL_DIR}/llama-194m"
          
          python -c "from sentence_transformers import SentenceTransformer; SentenceTransformer(\"sentence-transformers/all-roberta-large-v1\")"
          VARIANT=$(ls ~/.cache/huggingface/hub/models--sentence-transformers--all-roberta-large-v1/snapshots/)
          ln -s ~/.cache/huggingface/hub/models--sentence-transformers--all-roberta-large-v1/snapshots/"${VARIANT}" "${VLLM_SPYRE_TEST_MODEL_DIR}/all-roberta-large-v1"

      - name: "Run tests"
        env:
          MASTER_PORT: 12355
          MASTER_ADDR: localhost
          DISTRIBUTED_STRATEGY_IGNORE_MODULES: WordEmbedding
        run: |
          python -m pytest ${{ matrix.test_suite.flags }} \
            tests -v -k "${{ matrix.test_suite.tests }}"
