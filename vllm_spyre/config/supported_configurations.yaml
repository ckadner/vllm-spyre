# --8<-- [start:supported-model-runtime-configurations]

# Supported model runtime configurations:
#  - cb: True: continuous batching mode; False: static batching mode
#  - tp_size: [1|2|4|8|16] tensor parallel size
#  - max_model_len: context length
#  - max_num_seqs: number of sequences in a batch (per instance)
#  - warmup_shapes: [(fixed_prompt_length, max_new_tokens, batch_size)]

- model: "ibm-granite/granite-3.3-8b-instruct"
  configs: [
    { cb: False, tp_size: 1, warmup_shapes: [[2048, 1024, 16]] },
    { cb: False, tp_size: 4, warmup_shapes: [[2048, 1024, 16]] },
    { cb: False, tp_size: 4, warmup_shapes: [[6144, 2048,  1]] },
    { cb: False, tp_size: 4, warmup_shapes: [[7168, 1024,  1]] },
    { cb: True,  tp_size: 1, max_model_len: 3072, max_num_seqs: 16 },
    { cb: True,  tp_size: 4, max_model_len: 2048, max_num_seqs: 4 },
    { cb: True,  tp_size: 4, max_model_len: 4096, max_num_seqs: 4 },
    { cb: True,  tp_size: 4, max_model_len: 8192, max_num_seqs: 4 },
  ]
- model: "sentence-transformers/all-roberta-large-v1"
  configs: [
    { cb: False, tp_size: 1, warmup_shapes: [[64, 0, 4], [64, 0, 8], [128, 0, 4], [128, 0, 8]] },
  ]
# --8<-- [end:supported-model-runtime-configurations]
- model: "ibm-granite/granite-3.3-8b-instruct-FP8" #  "-FP8" ?
  configs: [
    #{ cb: True, tp_size: 1, max_model_len: 3072,  max_num_seqs: 16 }, # FUTURE
    #{ cb: True, tp_size: 4, max_model_len: 8192,  max_num_seqs: 4 },  # FUTURE
    #{ cb: True, tp_size: 4, max_model_len: 16384, max_num_seqs: 4 },  # FUTURE
    #{ cb: True, tp_size: 4, max_model_len: 32768, max_num_seqs: 4 },  # FUTURE
  ]
- model: "ibm-ai-platform/micro-g3.3-8b-instruct-1b"
  ignore: True
- model: "ibm-ai-platform/micro-g3.3-8b-instruct-1b-FP8"
  ignore: True

