# --8<-- [start:supported-model-runtime-configurations]

# Parameters:
#  - cb: True, for continuous batching; False, for static batching mode
#  - tp_size: tensor parallel size
#  - max_seq_len: context length (prompt_length + max_new_tokens)
#  - max_num_seqs: number of sequences in a batch (per instance)
#  - warmup_shapes: [(fixed_prompt_length, max_new_tokens, batch_size)]

- model: "ibm-granite/granite-3.3-8b-instruct"
  configs: [
    { cb: False, tp_size: 1, max_seq_len: 7168, max_prompt: 2048, max_num_seqs: 16 },
    { cb: True,  tp_size: 4, max_seq_len: 32768, max_num_seqs: 32 },
  ]
- model: "ibm-granite/granite-3.3-8b-instruct-FP8"
  configs: [
    { cb: True, tp_size: 4, max_seq_len: 32768, max_num_seqs: 32 },
  ]
- model: "ibm-granite/granite-embedding-125m-english"
  configs: [
    { cb: False, tp_size: 1, max_seq_len: 512, max_num_seqs: 64 },
  ]
- model: "ibm-granite/granite-embedding-278m-multilingual"
  configs: [
    { cb: False, tp_size: 1, max_seq_len: 512, max_num_seqs: 64 },
  ]
- model: "BAAI/bge-reranker-v2-m3"
  configs: [
    { cb: False, tp_size: 1, max_seq_len: 8192, max_num_seqs: 64 },
  ]
- model: "BAAI/bge-reranker-large"
  configs: [
    { cb: False, tp_size: 1, max_seq_len: 512, max_num_seqs: 64 },
  ]
- model: "sentence-transformers/all-roberta-large-v1"
  configs: [
    { cb: False, tp_size: 1, max_seq_len: 128, max_num_seqs: 8 },
  ]
# --8<-- [end:supported-model-runtime-configurations]
- model: "ibm-ai-platform/micro-g3.3-8b-instruct-1b"
  ignore: True
- model: "ibm-ai-platform/micro-g3.3-8b-instruct-1b-FP8"
  ignore: True
